# The Story of Evaluation  
A Beginner-Friendly Journey into Why Judging AI Is Harder Than Building It

ğŸŒ… Chapter 1: Aarav Meets the Overconfident Machine

Aarav was proud.

He had built his first AI application.
It could summarize articles.
Answer questions.
Even generate legal drafts.

It felt powerful.

Until one day,
a user emailed him.

â€œThe AI gave me wrong information.â€

Not obviously wrong.
Not gibberish.
Not nonsense.

Confidently wrong.

Thatâ€™s when Aarav realized something:

Building AI was easier than judging it.

---

ğŸŒ„ Chapter 2: The Problem With Smart Machines

At first, evaluating AI seemed simple.

If output = correct â†’ good.
If output = wrong â†’ bad.

That works for:
- Spam detection
- Image classification
- Fraud detection

But foundation models donâ€™t give fixed answers.

For one question,
there can be 100 valid responses.

Which one is â€œcorrectâ€?

There is no single ground truth.

And worse â€”
sometimes the wrong answer sounds perfect.

---

ğŸŒ¤ Chapter 3: The PhD Problem

Aarav read something that changed his perspective:

â€œItâ€™s easy to tell if a first graderâ€™s math is wrong.
Harder to judge a PhD-level proof.â€

AI models are becoming PhD-level.

To evaluate them, you might need:
- Domain expertise
- Fact-checking
- Deep reasoning

Evaluation is no longer a quick check.
Itâ€™s investigation.

And investigation takes time.

---

ğŸŒŠ Chapter 4: The Open-Ended Curse

Traditional machine learning was clean.

Input â†’ Output  
Compare with label â†’ Done.

But foundation models are open-ended.

Ask:
â€œSummarize this book.â€

There are infinite valid summaries.

You cannot create a list of all correct answers.

Evaluation breaks.

Aarav understood:
The open-ended nature of AI makes traditional evaluation useless.

---

ğŸ”¥ Chapter 5: The Black Box

Aarav wanted to understand why the model failed.

But he couldnâ€™t see:
- Training data
- Architecture details
- Internal reasoning

The model was a black box.

He could only judge it by outputs.

It was like evaluating a student
without knowing what they studied.

---

ğŸŒª Chapter 6: When Benchmarks Stop Working

Aarav learned about benchmarks:
GLUE.
SuperGLUE.
MMLU.

Models beat them quickly.

A benchmark becomes useless
once models get perfect scores.

Evaluation has to evolve.

And it must evolve fast.

---

ğŸŒ™ Chapter 7: The Temptation of Eyeballing

Some developers said:
â€œJust eyeball the outputs.â€
â€œLooks good to me.â€

But Aarav knew that was dangerous.

Confidence is not correctness.
Fluency is not truth.

Without systematic evaluation,
AI risk can outweigh AI benefit.

---

ğŸŒŒ Chapter 8: Measuring Uncertainty â€“ The Hidden Math

Before foundation models,
language models were evaluated using:

Entropy.
Cross entropy.
Perplexity.

Aarav simplified it in his mind:

Perplexity answers one question:

â€œHow confused is the model
when predicting the next word?â€

Lower perplexity â†’ more confident predictions.
Higher perplexity â†’ more uncertainty.

If a model has perplexity of 3,
itâ€™s like saying:
â€œIâ€™m choosing among 3 likely options.â€

Given vocabularies of 100,000 words,
thatâ€™s impressive.

But hereâ€™s the twist:

After alignment (RLHF, SFT),
perplexity can increase.

The model gets better at tasks,
but worse at raw prediction.

Evaluation isnâ€™t straightforward.

---

ğŸŒ… Chapter 9: The AI Judge

Then Aarav discovered something surprising.

To evaluate AI,
people started using AI.

AI as a judge.

One model generates.
Another scores.

But that creates a new question:

Can AI be trusted to evaluate AI?

The debate had begun.

---

ğŸŒ  Chapter 10: The Realization

Aarav finally understood.

Evaluation is not a metric.
It is a system.

You must:
- Identify failure points.
- Design metrics around risks.
- Build evaluation pipelines.
- Iterate continuously.

Because the smarter AI becomes,
the harder it is to judge.

And one day,
AI may become so advanced
that only the brightest humans
can evaluate it.

The challenge isnâ€™t building intelligence.

Itâ€™s measuring it.

---

Reference:  
AI Engineering â€“ Chip Huyen
